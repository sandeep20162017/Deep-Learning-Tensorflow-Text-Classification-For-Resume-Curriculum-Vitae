{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Tensorflow Text Classification For Resume/Curriculum Vitae  - Sandeep Kanao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Classification is the task of assigning the right label to a given piece of text. This text can either be a phrase, a sentence or even a paragraph. Our aim would be to take in some text as input and attach or assign a label to it. Since we will be using Tensor Flow deep learning library, we can call this the Tensorflow text classification system, and can be extended to extract information from resume or curriculum vitae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will go through how you can build your own text-based classifier with loads of classes or labels.\n",
    "\n",
    "The article Tensorflow text classification will be divided into multiple sections.\n",
    "First are the text pre-processing steps and creation and usage of the bag of words technique. \n",
    "\n",
    "Second is the training of the text classifier and finally the testing and using the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words – The Bag of Words model in Text Processing is the process of creating a unique list of words. This model is used as a tool for feature generation. \n",
    "\n",
    "Eg: consider two sentences:\n",
    "Star Wars is better than Star Trek.\n",
    "Star Trek isn’t as good as Star Wars.\n",
    "\n",
    "For the above two sentences, the bag of words will be: [“Star”, “Wars”, “Trek”, “better”, “good”, “isn’t”, “is”, “as”]. The position of each word in the list is hence fixed. \n",
    "\n",
    "Now, to construct a feature for classification from a sentence, we use a binary array ( an array where each element can either be 1 or 0).\n",
    "\n",
    "For example, a new sentence, “Wars is good” will be represented as [0,1,0,0,1,0,1,0] . As you can see in the array, position 2 is set to 1 because the word in position 2 is “wars” in the bag of words which is also present in our example sentence. This same holds good for the other words “is” and “good” as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import numpy as np\n",
    "import tflearn\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import json\n",
    "import string\n",
    "import unicodedata\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Data Preparation\n",
    "Before we train a model that can classify a given text to a particular category, we have to first prepare the data. We can create a simple JSON file that will hold the required data for training. - Sandeep Kanao\n",
    "\n",
    "Following is a sample file that I have created, that contains 9 categories. You can create how many ever categories that you want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{\n",
    "\t\"Education\": [\"Bachelor\", \"Master\", \"PhD\", \"High School\", \"College\", \"BSc\", \"B.Sc.\", \"MSc\", \"M.Sc\", \"BA\"],\n",
    "\t\"Email\": [\"sandeepkanao@gmail.com\", \"sandeep.kanao@abc.com\", \"abc123@mnc.net\"],\n",
    "\t\"Phone\": ['1-416-3040208', \"416-355 0208\", \"416 1220206\" \"41689700206\", \"304 123 4455\", \"1 123 456 5567\"],\n",
    "\t\"Skill\": [\"Python\", \"c++\", \"Java\", \"Angular\", \".net\"],\n",
    "\t\"Name\": [\"Sandeep.Kanao\", \"George M Very\", \"Jenefer Atkinson\", \"Kevin Spacy\"],\n",
    "        \"Address\": [\"17 street name\", \"#1204-191 College St.\", \"Apt 1290\", \"123 Main Parkway Vancouver BC\"],\n",
    "        \"StudiedAt\": [\"University of British Columbia\", \"University of\", \"Seneca College\", \"Famous College\"],\n",
    "\t\"WorkedAt\": [\"123 TECH\", \"BC Development\", \"Tech Works\", \"TD Canada\", \"Bank Of Montreal\"],\n",
    "\t\"Title\" : [\"Software Developer\", \"Programmer\", \"Architect\", \"Analyst\", \"Scientist\", \"Manager\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Data Load and Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Education': ['Bachelor', 'Master', 'PhD', 'High School', 'College', 'BSc', 'B.Sc.', 'MSc', 'M.Sc.', 'BA', 'Associate Degree', 'Associate of Arts (A.A.)', 'Associate of Science (A.S.)', \"Bachelor's (or Baccalaureate) Degree\", 'college program', 'Graduate Degree', 'Professional Degree', 'Joint Degrees', 'Liberal Arts and Career Combination', 'Teacher Certification', 'BA Hons', 'BFA', 'BJourn', 'BSA', 'BA', 'BEng', 'BHA', 'BTech', 'BArchSc', 'BComm'], 'Email': ['sandeepkanao@gmail.com', 'sandeep.kanao@abc.com', 'abc123@mnc.net', 'nick@gmail.com', 'vic@gmail.com', 'prince@yahoo.in', 'BillGates@gmail.com', 'SteveJobs@yahoo.com', 'BillJobs@aol.com', 'first.lastnumber@school.edu', 'john.smith01@harvard.edu', 'firstinitiallastnamerandomnumber@school.edu', 'jsmith1234@harvard.edu', 'noreply@labs.princeton.edu', 'whoever@harvard.edu', 'texasstars@yahoo.com', 'Fredblogs@thisdomain.com', 'Fredblogs@thisdomain.com'], 'Phone': ['1-416-3040208', '416-355 0208', '416 1220206', '41689700206', '304 123 4455', '1 123 456 5567', '919371424876', '+1 678 3390204', '15674456783', '567-980-3445', '890 342 3342', '011 1 989 678 4567', '900 789 8906', '18009009809'], 'Skill': ['Python', 'c++', 'Java', 'Angular', '.net'], 'Name': ['Sandeep.Kanao', 'George M Very', 'Jenefer Atkinson', 'Kevin Spacy', 'George Ing', 'Tim Jonson', 'Mery Wong', 'Kim Lee', 'Vikas Vats'], 'Address': ['17 street name', '#1204-191 College St.', 'Apt 1290', '123 Main Parkway Vancouver BC', 'Apartment 123', 'Suit 456', '#456 890 King Street', '89 Queen St.', '8900 Cres', 'Road', 'Street', 'St.', 'Apt', 'Apartment', 'Suit'], 'StudiedAt': ['University of British Columbia', 'University of', 'Seneca College', 'Famous College', 'University', 'College', 'High School', 'School'], 'WorkedAt': ['123 TECH', 'BC Development', 'Tech Works', 'TD Canada', 'Bank Of Montreal'], 'Title': ['Software Developer', 'Programmer', 'Architect', 'Analyst', 'Scientist', 'Manager']}\n",
      "Bachelor\n",
      "tokenized words:  ['Bachelor']\n",
      "Master\n",
      "tokenized words:  ['Master']\n",
      "PhD\n",
      "tokenized words:  ['PhD']\n",
      "High School\n",
      "tokenized words:  ['High', 'School']\n",
      "College\n",
      "tokenized words:  ['College']\n",
      "BSc\n",
      "tokenized words:  ['BSc']\n",
      "B.Sc.\n",
      "tokenized words:  ['B.Sc', '.']\n",
      "MSc\n",
      "tokenized words:  ['MSc']\n",
      "M.Sc.\n",
      "tokenized words:  ['M.Sc', '.']\n",
      "BA\n",
      "tokenized words:  ['BA']\n",
      "Associate Degree\n",
      "tokenized words:  ['Associate', 'Degree']\n",
      "Associate of Arts (A.A.)\n",
      "tokenized words:  ['Associate', 'of', 'Arts', '(', 'A.A', '.', ')']\n",
      "Associate of Science (A.S.)\n",
      "tokenized words:  ['Associate', 'of', 'Science', '(', 'A.S', '.', ')']\n",
      "Bachelor's (or Baccalaureate) Degree\n",
      "tokenized words:  ['Bachelor', \"'s\", '(', 'or', 'Baccalaureate', ')', 'Degree']\n",
      "college program\n",
      "tokenized words:  ['college', 'program']\n",
      "Graduate Degree\n",
      "tokenized words:  ['Graduate', 'Degree']\n",
      "Professional Degree\n",
      "tokenized words:  ['Professional', 'Degree']\n",
      "Joint Degrees\n",
      "tokenized words:  ['Joint', 'Degrees']\n",
      "Liberal Arts and Career Combination\n",
      "tokenized words:  ['Liberal', 'Arts', 'and', 'Career', 'Combination']\n",
      "Teacher Certification\n",
      "tokenized words:  ['Teacher', 'Certification']\n",
      "BA Hons\n",
      "tokenized words:  ['BA', 'Hons']\n",
      "BFA\n",
      "tokenized words:  ['BFA']\n",
      "BJourn\n",
      "tokenized words:  ['BJourn']\n",
      "BSA\n",
      "tokenized words:  ['BSA']\n",
      "BA\n",
      "tokenized words:  ['BA']\n",
      "BEng\n",
      "tokenized words:  ['BEng']\n",
      "BHA\n",
      "tokenized words:  ['BHA']\n",
      "BTech\n",
      "tokenized words:  ['BTech']\n",
      "BArchSc\n",
      "tokenized words:  ['BArchSc']\n",
      "BComm\n",
      "tokenized words:  ['BComm']\n",
      "sandeepkanao@gmail.com\n",
      "tokenized words:  ['sandeepkanao', '@', 'gmail.com']\n",
      "sandeep.kanao@abc.com\n",
      "tokenized words:  ['sandeep.kanao', '@', 'abc.com']\n",
      "abc123@mnc.net\n",
      "tokenized words:  ['abc123', '@', 'mnc.net']\n",
      "nick@gmail.com\n",
      "tokenized words:  ['nick', '@', 'gmail.com']\n",
      "vic@gmail.com\n",
      "tokenized words:  ['vic', '@', 'gmail.com']\n",
      "prince@yahoo.in\n",
      "tokenized words:  ['prince', '@', 'yahoo.in']\n",
      "BillGates@gmail.com\n",
      "tokenized words:  ['BillGates', '@', 'gmail.com']\n",
      "SteveJobs@yahoo.com\n",
      "tokenized words:  ['SteveJobs', '@', 'yahoo.com']\n",
      "BillJobs@aol.com\n",
      "tokenized words:  ['BillJobs', '@', 'aol.com']\n",
      "first.lastnumber@school.edu\n",
      "tokenized words:  ['first.lastnumber', '@', 'school.edu']\n",
      "john.smith01@harvard.edu\n",
      "tokenized words:  ['john.smith01', '@', 'harvard.edu']\n",
      "firstinitiallastnamerandomnumber@school.edu\n",
      "tokenized words:  ['firstinitiallastnamerandomnumber', '@', 'school.edu']\n",
      "jsmith1234@harvard.edu\n",
      "tokenized words:  ['jsmith1234', '@', 'harvard.edu']\n",
      "noreply@labs.princeton.edu\n",
      "tokenized words:  ['noreply', '@', 'labs.princeton.edu']\n",
      "whoever@harvard.edu\n",
      "tokenized words:  ['whoever', '@', 'harvard.edu']\n",
      "texasstars@yahoo.com\n",
      "tokenized words:  ['texasstars', '@', 'yahoo.com']\n",
      "Fredblogs@thisdomain.com\n",
      "tokenized words:  ['Fredblogs', '@', 'thisdomain.com']\n",
      "Fredblogs@thisdomain.com\n",
      "tokenized words:  ['Fredblogs', '@', 'thisdomain.com']\n",
      "1-416-3040208\n",
      "tokenized words:  ['1-416-3040208']\n",
      "416-355 0208\n",
      "tokenized words:  ['416-355', '0208']\n",
      "416 1220206\n",
      "tokenized words:  ['416', '1220206']\n",
      "41689700206\n",
      "tokenized words:  ['41689700206']\n",
      "304 123 4455\n",
      "tokenized words:  ['304', '123', '4455']\n",
      "1 123 456 5567\n",
      "tokenized words:  ['1', '123', '456', '5567']\n",
      "919371424876\n",
      "tokenized words:  ['919371424876']\n",
      "+1 678 3390204\n",
      "tokenized words:  ['+1', '678', '3390204']\n",
      "15674456783\n",
      "tokenized words:  ['15674456783']\n",
      "567-980-3445\n",
      "tokenized words:  ['567-980-3445']\n",
      "890 342 3342\n",
      "tokenized words:  ['890', '342', '3342']\n",
      "011 1 989 678 4567\n",
      "tokenized words:  ['011', '1', '989', '678', '4567']\n",
      "900 789 8906\n",
      "tokenized words:  ['900', '789', '8906']\n",
      "18009009809\n",
      "tokenized words:  ['18009009809']\n",
      "Python\n",
      "tokenized words:  ['Python']\n",
      "c++\n",
      "tokenized words:  ['c++']\n",
      "Java\n",
      "tokenized words:  ['Java']\n",
      "Angular\n",
      "tokenized words:  ['Angular']\n",
      ".net\n",
      "tokenized words:  ['.net']\n",
      "Sandeep.Kanao\n",
      "tokenized words:  ['Sandeep.Kanao']\n",
      "George M Very\n",
      "tokenized words:  ['George', 'M', 'Very']\n",
      "Jenefer Atkinson\n",
      "tokenized words:  ['Jenefer', 'Atkinson']\n",
      "Kevin Spacy\n",
      "tokenized words:  ['Kevin', 'Spacy']\n",
      "George Ing\n",
      "tokenized words:  ['George', 'Ing']\n",
      "Tim Jonson\n",
      "tokenized words:  ['Tim', 'Jonson']\n",
      "Mery Wong\n",
      "tokenized words:  ['Mery', 'Wong']\n",
      "Kim Lee\n",
      "tokenized words:  ['Kim', 'Lee']\n",
      "Vikas Vats\n",
      "tokenized words:  ['Vikas', 'Vats']\n",
      "17 street name\n",
      "tokenized words:  ['17', 'street', 'name']\n",
      "#1204-191 College St.\n",
      "tokenized words:  ['#', '1204-191', 'College', 'St', '.']\n",
      "Apt 1290\n",
      "tokenized words:  ['Apt', '1290']\n",
      "123 Main Parkway Vancouver BC\n",
      "tokenized words:  ['123', 'Main', 'Parkway', 'Vancouver', 'BC']\n",
      "Apartment 123\n",
      "tokenized words:  ['Apartment', '123']\n",
      "Suit 456\n",
      "tokenized words:  ['Suit', '456']\n",
      "#456 890 King Street\n",
      "tokenized words:  ['#', '456', '890', 'King', 'Street']\n",
      "89 Queen St.\n",
      "tokenized words:  ['89', 'Queen', 'St', '.']\n",
      "8900 Cres\n",
      "tokenized words:  ['8900', 'Cres']\n",
      "Road\n",
      "tokenized words:  ['Road']\n",
      "Street\n",
      "tokenized words:  ['Street']\n",
      "St.\n",
      "tokenized words:  ['St', '.']\n",
      "Apt\n",
      "tokenized words:  ['Apt']\n",
      "Apartment\n",
      "tokenized words:  ['Apartment']\n",
      "Suit\n",
      "tokenized words:  ['Suit']\n",
      "University of British Columbia\n",
      "tokenized words:  ['University', 'of', 'British', 'Columbia']\n",
      "University of\n",
      "tokenized words:  ['University', 'of']\n",
      "Seneca College\n",
      "tokenized words:  ['Seneca', 'College']\n",
      "Famous College\n",
      "tokenized words:  ['Famous', 'College']\n",
      "University\n",
      "tokenized words:  ['University']\n",
      "College\n",
      "tokenized words:  ['College']\n",
      "High School\n",
      "tokenized words:  ['High', 'School']\n",
      "School\n",
      "tokenized words:  ['School']\n",
      "123 TECH\n",
      "tokenized words:  ['123', 'TECH']\n",
      "BC Development\n",
      "tokenized words:  ['BC', 'Development']\n",
      "Tech Works\n",
      "tokenized words:  ['Tech', 'Works']\n",
      "TD Canada\n",
      "tokenized words:  ['TD', 'Canada']\n",
      "Bank Of Montreal\n",
      "tokenized words:  ['Bank', 'Of', 'Montreal']\n",
      "Software Developer\n",
      "tokenized words:  ['Software', 'Developer']\n",
      "Programmer\n",
      "tokenized words:  ['Programmer']\n",
      "Architect\n",
      "tokenized words:  ['Architect']\n",
      "Analyst\n",
      "tokenized words:  ['Analyst']\n",
      "Scientist\n",
      "tokenized words:  ['Scientist']\n",
      "Manager\n",
      "tokenized words:  ['Manager']\n",
      "['#', \"'s\", '(', ')', '+1', '.', '.net', '011', '0208', '1', '1-416-3040208', '1204-191', '1220206', '123', '1290', '15674456783', '17', '18009009809', '304', '3342', '3390204', '342', '416', '416-355', '41689700206', '4455', '456', '4567', '5567', '567-980-3445', '678', '789', '89', '890', '8900', '8906', '900', '919371424876', '989', '@', 'a.', 'a.s', 'abc.com', 'abc123', 'analyst', 'and', 'angul', 'aol.com', 'apart', 'apt', 'architect', 'art', 'assocy', 'atkinson', 'b.sc', 'ba', 'baccala', 'bachel', 'bank', 'barchsc', 'bc', 'bcom', 'beng', 'bfa', 'bha', 'billg', 'billjob', 'bjourn', 'brit', 'bsa', 'bsc', 'btech', 'c++', 'canad', 'car', 'cert', 'colleg', 'columb', 'combin', 'cre', 'degr', 'develop', 'fam', 'first.lastnumber', 'firstinitiallastnamerandomnumb', 'fredblog', 'georg', 'gmail.com', 'gradu', 'harvard.edu', 'high', 'hon', 'ing', 'jav', 'jenef', 'john.smith01', 'joint', 'jonson', 'jsmith1234', 'kevin', 'kim', 'king', 'labs.princeton.edu', 'lee', 'lib', 'm', 'm.sc', 'main', 'man', 'mast', 'mery', 'mnc.net', 'mont', 'msc', 'nam', 'nick', 'noreply', 'of', 'or', 'parkway', 'phd', 'print', 'profess', 'program', 'python', 'queen', 'road', 'sandeep.kanao', 'sandeepkanao', 'school', 'school.edu', 'sci', 'senec', 'softw', 'spacy', 'st', 'stevejob', 'street', 'suit', 'td', 'teach', 'tech', 'texasst', 'thisdomain.com', 'tim', 'univers', 'vancouv', 'vat', 'very', 'vic', 'vika', 'whoev', 'wong', 'work', 'yahoo.com', 'yahoo.in']\n",
      "[(['Bachelor'], 'Education'), (['Master'], 'Education'), (['PhD'], 'Education'), (['High', 'School'], 'Education'), (['College'], 'Education'), (['BSc'], 'Education'), (['B.Sc', '.'], 'Education'), (['MSc'], 'Education'), (['M.Sc', '.'], 'Education'), (['BA'], 'Education'), (['Associate', 'Degree'], 'Education'), (['Associate', 'of', 'Arts', '(', 'A.A', '.', ')'], 'Education'), (['Associate', 'of', 'Science', '(', 'A.S', '.', ')'], 'Education'), (['Bachelor', \"'s\", '(', 'or', 'Baccalaureate', ')', 'Degree'], 'Education'), (['college', 'program'], 'Education'), (['Graduate', 'Degree'], 'Education'), (['Professional', 'Degree'], 'Education'), (['Joint', 'Degrees'], 'Education'), (['Liberal', 'Arts', 'and', 'Career', 'Combination'], 'Education'), (['Teacher', 'Certification'], 'Education'), (['BA', 'Hons'], 'Education'), (['BFA'], 'Education'), (['BJourn'], 'Education'), (['BSA'], 'Education'), (['BA'], 'Education'), (['BEng'], 'Education'), (['BHA'], 'Education'), (['BTech'], 'Education'), (['BArchSc'], 'Education'), (['BComm'], 'Education'), (['sandeepkanao', '@', 'gmail.com'], 'Email'), (['sandeep.kanao', '@', 'abc.com'], 'Email'), (['abc123', '@', 'mnc.net'], 'Email'), (['nick', '@', 'gmail.com'], 'Email'), (['vic', '@', 'gmail.com'], 'Email'), (['prince', '@', 'yahoo.in'], 'Email'), (['BillGates', '@', 'gmail.com'], 'Email'), (['SteveJobs', '@', 'yahoo.com'], 'Email'), (['BillJobs', '@', 'aol.com'], 'Email'), (['first.lastnumber', '@', 'school.edu'], 'Email'), (['john.smith01', '@', 'harvard.edu'], 'Email'), (['firstinitiallastnamerandomnumber', '@', 'school.edu'], 'Email'), (['jsmith1234', '@', 'harvard.edu'], 'Email'), (['noreply', '@', 'labs.princeton.edu'], 'Email'), (['whoever', '@', 'harvard.edu'], 'Email'), (['texasstars', '@', 'yahoo.com'], 'Email'), (['Fredblogs', '@', 'thisdomain.com'], 'Email'), (['Fredblogs', '@', 'thisdomain.com'], 'Email'), (['1-416-3040208'], 'Phone'), (['416-355', '0208'], 'Phone'), (['416', '1220206'], 'Phone'), (['41689700206'], 'Phone'), (['304', '123', '4455'], 'Phone'), (['1', '123', '456', '5567'], 'Phone'), (['919371424876'], 'Phone'), (['+1', '678', '3390204'], 'Phone'), (['15674456783'], 'Phone'), (['567-980-3445'], 'Phone'), (['890', '342', '3342'], 'Phone'), (['011', '1', '989', '678', '4567'], 'Phone'), (['900', '789', '8906'], 'Phone'), (['18009009809'], 'Phone'), (['Python'], 'Skill'), (['c++'], 'Skill'), (['Java'], 'Skill'), (['Angular'], 'Skill'), (['.net'], 'Skill'), (['Sandeep.Kanao'], 'Name'), (['George', 'M', 'Very'], 'Name'), (['Jenefer', 'Atkinson'], 'Name'), (['Kevin', 'Spacy'], 'Name'), (['George', 'Ing'], 'Name'), (['Tim', 'Jonson'], 'Name'), (['Mery', 'Wong'], 'Name'), (['Kim', 'Lee'], 'Name'), (['Vikas', 'Vats'], 'Name'), (['17', 'street', 'name'], 'Address'), (['#', '1204-191', 'College', 'St', '.'], 'Address'), (['Apt', '1290'], 'Address'), (['123', 'Main', 'Parkway', 'Vancouver', 'BC'], 'Address'), (['Apartment', '123'], 'Address'), (['Suit', '456'], 'Address'), (['#', '456', '890', 'King', 'Street'], 'Address'), (['89', 'Queen', 'St', '.'], 'Address'), (['8900', 'Cres'], 'Address'), (['Road'], 'Address'), (['Street'], 'Address'), (['St', '.'], 'Address'), (['Apt'], 'Address'), (['Apartment'], 'Address'), (['Suit'], 'Address'), (['University', 'of', 'British', 'Columbia'], 'StudiedAt'), (['University', 'of'], 'StudiedAt'), (['Seneca', 'College'], 'StudiedAt'), (['Famous', 'College'], 'StudiedAt'), (['University'], 'StudiedAt'), (['College'], 'StudiedAt'), (['High', 'School'], 'StudiedAt'), (['School'], 'StudiedAt'), (['123', 'TECH'], 'WorkedAt'), (['BC', 'Development'], 'WorkedAt'), (['Tech', 'Works'], 'WorkedAt'), (['TD', 'Canada'], 'WorkedAt'), (['Bank', 'Of', 'Montreal'], 'WorkedAt'), (['Software', 'Developer'], 'Title'), (['Programmer'], 'Title'), (['Architect'], 'Title'), (['Analyst'], 'Title'), (['Scientist'], 'Title'), (['Manager'], 'Title')]\n"
     ]
    }
   ],
   "source": [
    "# a table structure to hold the different punctuation used\n",
    "tbl = dict.fromkeys(i for i in range(sys.maxunicode)\n",
    "                    if unicodedata.category(chr(i)).startswith('P'))\n",
    "\n",
    "\n",
    "# method to remove punctuations from sentences.\n",
    "def remove_punctuation(text):\n",
    "    return text\n",
    "    #return text.translate(tbl)\n",
    "\n",
    "# initialize the stemmer\n",
    "stemmer = LancasterStemmer()\n",
    "# variable to hold the Json data read from the file\n",
    "data = None\n",
    "\n",
    "# read the json file and load the training data\n",
    "with open('traindata.json') as json_data:\n",
    "    data = json.load(json_data)\n",
    "    print(data)\n",
    "\n",
    "# get a list of all categories to train for\n",
    "categories = list(data.keys())\n",
    "words = []\n",
    "# a list of tuples with words in the sentence and category name\n",
    "docs = []\n",
    "\n",
    "for each_category in data.keys():\n",
    "    for each_sentence in data[each_category]:\n",
    "        # remove any punctuation from the sentence\n",
    "        each_sentence = remove_punctuation(each_sentence)\n",
    "        print(each_sentence)\n",
    "        # extract words from each sentence and append to the word list\n",
    "        w = nltk.word_tokenize(each_sentence)\n",
    "        print(\"tokenized words: \", w)\n",
    "        words.extend(w)\n",
    "        docs.append((w, each_category))\n",
    "\n",
    "# stem and lower each word and remove duplicates\n",
    "words = [stemmer.stem(w.lower()) for w in words]\n",
    "words = sorted(list(set(words)))\n",
    "\n",
    "print(words)\n",
    "print(docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Convert the data to Tensorflow Specification - Sandeep Kanao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our training data\n",
    "training = []\n",
    "output = []\n",
    "# create an empty array for our output\n",
    "output_empty = [0] * len(categories)\n",
    "\n",
    "\n",
    "for doc in docs:\n",
    "    # initialize our bag of words(bow) for each document in the list\n",
    "    bow = []\n",
    "    # list of tokenized words for the pattern\n",
    "    token_words = doc[0]\n",
    "    # stem each word\n",
    "    token_words = [stemmer.stem(word.lower()) for word in token_words]\n",
    "    # create our bag of words array\n",
    "    for w in words:\n",
    "        bow.append(1) if w in token_words else bow.append(0)\n",
    "\n",
    "    output_row = list(output_empty)\n",
    "    output_row[categories.index(doc[1])] = 1\n",
    "\n",
    "    # our training set will contain a the bag of words model and the output row that tells\n",
    "    # which catefory that bow belongs to.\n",
    "    training.append([bow, output_row])\n",
    "\n",
    "# shuffle our features and turn into np.array as tensorflow  takes in numpy array\n",
    "random.shuffle(training)\n",
    "training = np.array(training)\n",
    "\n",
    "# trainX contains the Bag of words and train_y contains the label/ category\n",
    "train_x = list(training[:, 0])\n",
    "train_y = list(training[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Initiate Tensorflow Text Classification - Sandeep Kanao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below runs for a 1000 epochs. I ran it for 10,000 epochs which had 30,000 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 699899  | total loss: \u001b[1m\u001b[32m0.02096\u001b[0m\u001b[0m | time: 0.060s\n",
      "\u001b[2K\r",
      "| Adam | epoch: 24997 | loss: 0.02096 - acc: 0.9905 -- iter: 044/110\n"
     ]
    }
   ],
   "source": [
    "## reset underlying graph data\n",
    "tf.reset_default_graph()\n",
    "# Build neural network\n",
    "net = tflearn.input_data(shape=[None, len(train_x[0])])\n",
    "net = tflearn.fully_connected(net, 24)\n",
    "net = tflearn.fully_connected(net, 24)\n",
    "net = tflearn.fully_connected(net, len(train_y[0]), activation='softmax')\n",
    "net = tflearn.regression(net)\n",
    "\n",
    "# Define model and setup tensorboard\n",
    "model = tflearn.DNN(net, tensorboard_dir='tflearn_logs')\n",
    "# Start training (apply gradient descent algorithm)\n",
    "model.fit(train_x, train_y, n_epoch=25000, batch_size=4, show_metric=True)\n",
    "model.save('model.tflearn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Testing the Tensorflow Text Classification Model -Sandeep Kanao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email\n",
      "Address\n",
      "Title\n",
      "StudiedAt\n",
      "StudiedAt\n",
      "StudiedAt\n",
      "StudiedAt\n",
      "StudiedAt\n",
      "Address\n",
      "WorkedAt\n",
      "Title\n"
     ]
    }
   ],
   "source": [
    "# let's test the mdodel for a few sentences:\n",
    "# the first two sentences are used for training, and the last two sentences are not present in the training data.\n",
    "sent_1 = \"sandeepkanao@gmail.com\"\n",
    "sent_2 = \"123 Main Parkway Vancouver BC\"\n",
    "sent_3 = \"B.Sc. Computer Science\"\n",
    "sent_4 = \"University of New York\"\n",
    "sent_5 = \"jonsmith@mail.com\"\n",
    "sent_6 = \"780678709\"\n",
    "sent_7 = \"ASP.NET\"\n",
    "sent_8 = \"Jon Smith\"\n",
    "sent_9 = \"17 MNC Cres Toronto ON\"\n",
    "sent_10 = \"2013 - Present Royal Bank Of Canada\"\n",
    "sent_11 = \"2012 - 2013 Vancouver, BC Software Developer BC Development\"\n",
    "\n",
    "# a method that takes in a sentence and list of all words\n",
    "# and returns the data in a form the can be fed to tensorflow\n",
    "\n",
    "\n",
    "def get_tf_record(sentence):\n",
    "    global words\n",
    "    # tokenize the pattern\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    # stem each word\n",
    "    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
    "    # bag of words\n",
    "    bow = [0]*len(words)\n",
    "    for s in sentence_words:\n",
    "        for i, w in enumerate(words):\n",
    "            if w == s:\n",
    "                bow[i] = 1\n",
    "\n",
    "    return(np.array(bow))\n",
    "\n",
    "\n",
    "# we can start to predict the results for each of the sentences\n",
    "print(categories[np.argmax(model.predict([get_tf_record(sent_1)]))])\n",
    "print(categories[np.argmax(model.predict([get_tf_record(sent_2)]))])\n",
    "print(categories[np.argmax(model.predict([get_tf_record(sent_3)]))])\n",
    "print(categories[np.argmax(model.predict([get_tf_record(sent_4)]))])\n",
    "print(categories[np.argmax(model.predict([get_tf_record(sent_5)]))])\n",
    "print(categories[np.argmax(model.predict([get_tf_record(sent_6)]))])\n",
    "print(categories[np.argmax(model.predict([get_tf_record(sent_7)]))])\n",
    "print(categories[np.argmax(model.predict([get_tf_record(sent_8)]))])\n",
    "print(categories[np.argmax(model.predict([get_tf_record(sent_9)]))])\n",
    "print(categories[np.argmax(model.predict([get_tf_record(sent_10)]))])\n",
    "print(categories[np.argmax(model.predict([get_tf_record(sent_11)]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
